{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports  and Required Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run AttaxGame.ipynb\n",
    "%run AlphaZero_Attax.ipynb\n",
    "%run NeuralNetwork_Attax.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## AlphaZero Agent for Attax Game\n",
    "\n",
    "This notebook implements an intelligent agent that utilizes the AlphaZero algorithm to play the Attax game. The `AlphaZeroAgent` class encapsulates the logic required for the agent to interact with the game environment, make strategic decisions, and learn from its experiences.\n",
    "\n",
    "Key components of the `AlphaZeroAgent` class include:\n",
    "\n",
    "- **Initialization**: Set up the agent with the specific game it will play and load the pre-trained AlphaZero model.\n",
    "- **Model Loading**: Depending on the game dimensions (4x4, 5x5, or 6x6), load the corresponding neural network model trained to evaluate game states and generate probable moves.\n",
    "- **Move Making**: Use a Monte Carlo Tree Search (MCTS) guided by the neural network to simulate games and decide on the most promising move.\n",
    "- **Response Handling**: Process responses from the game server, which includes updating the game state based on opponent moves and handling end-game scenarios.\n",
    "\n",
    "The following code block defines the `AlphaZeroAgent` class and its critical methods. It is designed to integrate seamlessly with the Attax game server, receiving game updates, and sending moves back in a predefined communication protocol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroAgent:\n",
    "    \n",
    "    \"\"\"\n",
    "    Agent class that uses the AlphaZero algorithm to play games.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Game):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the AlphaZero agent with the game it will play.\n",
    "        \"\"\"\n",
    "\n",
    "        self.Game = Game\n",
    "        self.alpha_zero = self.load_alpha_zero()\n",
    "        self.turn = -1  # Keeps track of whose turn it is (-1 for opponent, 1 for agent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_alpha_zero(self):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Loads the appropriate AlphaZero model based on the game being played.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        game = AttaxGame()  # Initializes the game\n",
    "\n",
    "        # Load the model file corresponding to the game size\n",
    "        model_files = {\n",
    "            \"A4x4\": '/Users/larasousa/Desktop/uni/labiacd/Attax/model8Attax4.pt',\n",
    "            \"A5x5\": '/Users/larasousa/Desktop/uni/labiacd/Attax/model14Attax5.pt',\n",
    "            \"A6x6\": '/Users/larasousa/Desktop/uni/labiacd/Attax/model7Attax6.pt',\n",
    "        }\n",
    "        model_file = model_files.get(self.Game, \"Unknown game type\")\n",
    "        \n",
    "        # Check if GPU is available and set the device accordingly\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Set the game board size based on the game type\n",
    "        game_sizes = {\"A4x4\": 4, \"A5x5\": 5, \"A6x6\": 6}\n",
    "        game.N = game_sizes.get(self.Game)\n",
    "\n",
    "        # Initialize the game state and model\n",
    "        game.action_size = game.N * game.N\n",
    "        state = game.get_initial_state()\n",
    "        model = ResNet(game, state, 1, 4, 64, device)\n",
    "        model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Create an AlphaZero instance with the loaded model and other dependencies\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "        # Training parameters\n",
    "        args = {\n",
    "            'C': 2, # Exploration constant for the UCT formula\n",
    "            'num_searches': 100, # Number of searches to run for each move\n",
    "            'num_iterations': 500, # Number of iterations to run the training\n",
    "            'num_selfPlay_iterations': 1000, # Number of self play iterations\n",
    "            'num_epochs': 50, # Number of epochs to run the training\n",
    "            'batch_size': 64, # Batch size\n",
    "            'epsilon': 0.25, # Epsilon for the Dirichlet noise\n",
    "            'alpha': 0.3 # Alpha for the Dirichlet noise\n",
    "        }\n",
    "\n",
    "        az = AlphaZero(model, optimizer, game, args)\n",
    "        return az\n",
    "\n",
    "\n",
    "\n",
    "    def make_move(self, agent):\n",
    "        \"\"\"\n",
    "        Determines the move to be made by the AlphaZero agent.\n",
    "        \"\"\"\n",
    "        # Update the turn based on the agent number\n",
    "        self.turn = 1 if agent == 1 else -1\n",
    "\n",
    "        # Perform self-play to determine the next move\n",
    "        memory = self.alpha_zero.selfPlay()\n",
    "\n",
    "        # Select the last state and action probabilities from memory\n",
    "        last_state, last_action_probs, _ = memory[-1]\n",
    "\n",
    "        # Choose an action based on the probabilities\n",
    "        action = np.random.choice(list(last_action_probs.keys()), p=list(last_action_probs.values()))\n",
    "        return action\n",
    "\n",
    "    def receive_response(self, response, ag):\n",
    "        \"\"\"\n",
    "        Processes the server's response and updates the game state accordingly.\n",
    "        \"\"\"\n",
    "        # Update the turn based on the agent number\n",
    "        self.turn = -1 if ag == 1 else 1\n",
    "\n",
    "        # Process the server's response, updating the game state or handling game end\n",
    "        if 'MOVE' in response:\n",
    "            # Parse the move from the response\n",
    "            _, move_str = response.split(' ')\n",
    "            coords = move_str.split(',')\n",
    "            if len(coords) == 4:\n",
    "                # Update the game state with the received move\n",
    "                move_formatted = f\"{coords[0]}{coords[1]}_{coords[2]}{coords[3]}\"\n",
    "                self.alpha_zero.update_state(move_formatted)\n",
    "            else:\n",
    "                print(\"Unexpected response format:\", response)\n",
    "\n",
    "        if 'END GAME' in response:\n",
    "            # Handle the end of the game\n",
    "            return False\n",
    "\n",
    "        # Continue the game if no end condition is met\n",
    "        return True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
